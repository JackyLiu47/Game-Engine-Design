{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackyLiu47/Game-Engine-Design/blob/master/memeBlip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMY3KaHIVkk9"
      },
      "outputs": [],
      "source": [
        "!nvcc --version\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install yacs\n",
        "!pip install torch transformers pytorch-lightning\n",
        "!pip install --upgrade pytorch-lightning optuna optuna-integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C8DGyiatQjx"
      },
      "outputs": [],
      "source": [
        "# !pip install numba\n",
        "\n",
        "# from numba import cuda\n",
        "# device = cuda.get_current_device()\n",
        "# device.reset()\n",
        "import torch\n",
        "\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
        "print(\"Current Device:\", torch.cuda.current_device())\n",
        "print(\"Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzcJWuAMbfZv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "from yacs.config import CfgNode\n",
        "import numpy as np\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "else:\n",
        "    torch.set_default_tensor_type(torch.FloatTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h0ell6HMrV6"
      },
      "outputs": [],
      "source": [
        "class Custom_Dataset(Dataset):\n",
        "    def __init__(self, cfg, root_folder, dataset, label, split='train', image_size=224, fast=True):\n",
        "        super(Custom_Dataset, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        self.root_folder = root_folder\n",
        "        self.dataset = dataset\n",
        "        self.split = split\n",
        "        self.label = label\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.fast = fast\n",
        "\n",
        "        self.info_file = cfg.info_file\n",
        "        self.df = pd.read_csv(self.info_file)\n",
        "        self.df = self.df[self.df['split'] == self.split].reset_index(drop=True)\n",
        "\n",
        "        if self.label == 'target':\n",
        "            self.df = self.df[self.df['hate'] == 1].reset_index(drop=True)\n",
        "\n",
        "        float_cols = self.df.select_dtypes(float).columns\n",
        "        self.df[float_cols] = self.df[float_cols].fillna(-1).astype('Int64')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        if row['text'] == 'None':\n",
        "            text = 'null'\n",
        "        else:\n",
        "            text = row['text']\n",
        "\n",
        "        image_fn = row['name']\n",
        "        try:\n",
        "            image = Image.open(f\"{self.cfg.img_folder}/{image_fn}\").convert('RGB')\n",
        "            image = image.resize((self.image_size, self.image_size))\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error loading image {image_fn}: {e}\")\n",
        "\n",
        "        item = {\n",
        "        'image': image,\n",
        "        'text': text,\n",
        "        'label': row[self.label],\n",
        "        'idx_meme': row['name'],\n",
        "        }\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcAl7xF7XPt0"
      },
      "outputs": [],
      "source": [
        "from transformers import BlipProcessor, BlipForImageTextRetrieval, BlipConfig\n",
        "\n",
        "class MemeBLIP_Collator:\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "        # Modify configuration\n",
        "        blipconfig = BlipConfig.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n",
        "        blipconfig.max_position_embeddings = 2048  # Extend max length if needed\n",
        "        # Initialize BLIP model and processor\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n",
        "        self.blip_model = BlipForImageTextRetrieval.from_pretrained(\n",
        "            \"Salesforce/blip-itm-large-coco\", config=blipconfig\n",
        "        ).to(self.cfg.device)\n",
        "        self.blip_model.eval()\n",
        "\n",
        "    def split_text_into_chunks(self, text, max_length):\n",
        "        \"\"\"\n",
        "        Splits a long text into chunks of size `max_length`.\n",
        "        \"\"\"\n",
        "        tokens = self.processor.tokenizer(\n",
        "            text, return_tensors=\"pt\", truncation=False, add_special_tokens=False\n",
        "        )[\"input_ids\"].squeeze(0).tolist()\n",
        "        return [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # Prepare feature lists\n",
        "        image_features_list = []\n",
        "        text_features_list = []\n",
        "\n",
        "        labels = torch.LongTensor([item['label'] for item in batch]).to(self.cfg.device)\n",
        "        idx_memes = [item['idx_meme'] for item in batch]\n",
        "\n",
        "        batch_new = {\n",
        "            'labels': labels,\n",
        "            'idx_memes': idx_memes,\n",
        "        }\n",
        "\n",
        "        for item in batch:\n",
        "            # Process image\n",
        "            processed_image = self.processor(\n",
        "                images=item['image'], return_tensors=\"pt\"\n",
        "            )[\"pixel_values\"].to(self.cfg.device)\n",
        "\n",
        "            # Process text: Split into chunks\n",
        "            text_chunks = self.split_text_into_chunks(item['text'], max_length=512)\n",
        "            text_features = []\n",
        "\n",
        "            for chunk in text_chunks:\n",
        "                processed_text = {\n",
        "                    \"input_ids\": torch.tensor([chunk]).to(self.cfg.device),\n",
        "                    \"attention_mask\": torch.ones(len(chunk)).unsqueeze(0).to(self.cfg.device)\n",
        "                }\n",
        "\n",
        "                # Extract features for each chunk\n",
        "                _, chunk_features = self.compute_features(\n",
        "                    {\"pixel_values\": processed_image, **processed_text}\n",
        "                )\n",
        "                text_features.append(chunk_features)\n",
        "\n",
        "            # Aggregate features from chunks\n",
        "            text_features = torch.mean(torch.stack(text_features), dim=0)\n",
        "\n",
        "            # Extract image features\n",
        "            image_features, _ = self.compute_features(\n",
        "                {\"pixel_values\": processed_image, **processed_text}\n",
        "            )\n",
        "\n",
        "            # Collect features\n",
        "            image_features_list.append(image_features.cpu().detach())\n",
        "            text_features_list.append(text_features.cpu().detach())\n",
        "\n",
        "        # Combine features\n",
        "        batch_new['image_features'] = torch.cat(image_features_list, dim=0).to(self.cfg.device)\n",
        "        batch_new['text_features'] = torch.cat(text_features_list, dim=0).to(self.cfg.device)\n",
        "\n",
        "        return batch_new\n",
        "\n",
        "    def compute_features(self, processed):\n",
        "        # Extract visual and text features from BLIP model\n",
        "        vision_output = self.blip_model.vision_model(processed['pixel_values'])\n",
        "        image_features = vision_output.pooler_output.to(self.cfg.device)  # Image features\n",
        "        text_output = self.blip_model.text_encoder(\n",
        "            input_ids=processed['input_ids'],\n",
        "            attention_mask=processed['attention_mask']\n",
        "        )\n",
        "        text_features = text_output.last_hidden_state.mean(dim=1).to(self.cfg.device)  # Text features\n",
        "\n",
        "        return image_features, text_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm5SzqC1OnoQ"
      },
      "outputs": [],
      "source": [
        "def load_dataset(cfg, split):\n",
        "    dataset = Custom_Dataset(\n",
        "        cfg=cfg,\n",
        "        root_folder=cfg.root_dir,\n",
        "        dataset=cfg.dataset_name,\n",
        "        split=split,\n",
        "        image_size=cfg.image_size,\n",
        "        label=cfg.label,\n",
        "        fast=cfg.fast_process\n",
        "    )\n",
        "    collator = Custom_Collator(cfg)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz9AY3ODOryZ"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(cfg, split=\"train\"):\n",
        "    dataset = load_dataset(cfg, split)\n",
        "    subset_size = int(len(dataset) * 0.3)  # 使用 30% 的数据\n",
        "    subset_indices = np.random.choice(len(dataset), subset_size, replace=False)\n",
        "\n",
        "    # 创建子集\n",
        "    subset = Subset(dataset, subset_indices)\n",
        "    collator = MemeBLIP_Collator(cfg)\n",
        "    generator = torch.Generator(device=\"cuda\") if torch.cuda.is_available() else torch.Generator()\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=(split == \"train\"),\n",
        "        generator = generator,\n",
        "        collate_fn=collator\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKpe3VHHO0Cw"
      },
      "outputs": [],
      "source": [
        "cfg = CfgNode()\n",
        "\n",
        "# 路径设置\n",
        "cfg.root_dir = './'\n",
        "cfg.img_folder = '/content/drive/MyDrive/Colab_Notebooks/MemeCLIP-main/dataset/Images'\n",
        "cfg.info_file = '/content/drive/MyDrive/Colab_Notebooks/MemeCLIP-main/dataset/PrideMM.csv'\n",
        "cfg.checkpoint_path = os.path.join(cfg.root_dir, 'checkpoints')\n",
        "cfg.checkpoint_file = os.path.join(cfg.checkpoint_path, 'model.ckpt')\n",
        "\n",
        "# 模型与数据集设置\n",
        "cfg.clip_variant = \"ViT-L/14\"\n",
        "cfg.dataset_name = 'Pride'\n",
        "cfg.name = 'MemeBLIP'\n",
        "cfg.label = 'hate'\n",
        "cfg.seed = 42\n",
        "cfg.test_only = False\n",
        "cfg.device = 'cuda'\n",
        "cfg.gpus = [0]\n",
        "\n",
        "# 根据任务类型动态设置类别\n",
        "if cfg.label == 'hate':\n",
        "    cfg.class_names = ['Benign Meme', 'Harmful Meme']\n",
        "elif cfg.label == 'humour':\n",
        "    cfg.class_names = ['No Humour', 'Humour']\n",
        "elif cfg.label == 'target':\n",
        "    cfg.class_names = ['No particular target', 'Individual', 'Community', 'Organization']\n",
        "elif cfg.label == 'stance':\n",
        "    cfg.class_names = ['Neutral', 'Support', 'Oppose']\n",
        "\n",
        "# 超参数设置\n",
        "cfg.batch_size = 16\n",
        "cfg.image_size = 224\n",
        "cfg.num_mapping_layers = 1\n",
        "cfg.unmapped_dim = 768\n",
        "cfg.map_dim = 1024\n",
        "cfg.num_pre_output_layers = 2\n",
        "cfg.drop_probs = [0.4, 0.2, 0.3]\n",
        "cfg.dropout_rate = 0.5\n",
        "cfg.hidden_dim = 1024\n",
        "cfg.lr = 1e-4\n",
        "cfg.max_epochs = 20\n",
        "cfg.weight_decay = 1e-4\n",
        "cfg.num_classes = len(cfg.class_names)\n",
        "cfg.scale = 30\n",
        "cfg.print_model = True\n",
        "cfg.fast_process = True\n",
        "cfg.reproduce = False\n",
        "cfg.ratio = 0.7\n",
        "cfg.num_layers = 3\n",
        "cfg.activation = 'ReLU'\n",
        "cfg.hidden_dim1 = 1024\n",
        "\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko7aOwsIO8ch"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "data_file = \"/content/drive/MyDrive/Colab_Notebooks/MemeCLIP-main/dataset/PrideMM.csv\"\n",
        "df = pd.read_csv(data_file)\n",
        "print(df.columns)\n",
        "\n",
        "# 加载训练和验证数据\n",
        "train_loader = create_dataloader(cfg, split=\"train\")\n",
        "val_loader = create_dataloader(cfg, split=\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV891g7wF82U"
      },
      "outputs": [],
      "source": [
        "class LinearProjection(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_layers, drop_probs):\n",
        "        super(LinearProjection, self).__init__()\n",
        "\n",
        "        # 如果 drop_probs 是列表，取第一个值；如果是标量，直接使用\n",
        "        if isinstance(drop_probs, list):\n",
        "            dropout_prob = drop_probs[0]\n",
        "        else:\n",
        "            dropout_prob = drop_probs\n",
        "\n",
        "        map_layers = [nn.Linear(input_dim, output_dim),\n",
        "                      nn.Dropout(p=dropout_prob)]\n",
        "\n",
        "        for _ in range(1, num_layers):\n",
        "            map_layers.extend([\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(output_dim, output_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=dropout_prob)\n",
        "            ])\n",
        "\n",
        "        self.proj = nn.Sequential(*map_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, c_in, reduction=4, dropout_rate=0.3):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(c_in, c_in // reduction, bias=False),\n",
        "            nn.BatchNorm1d(c_in // reduction),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(c_in // reduction, c_in, bias=False),\n",
        "            nn.BatchNorm1d(c_in),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=dropout_rate)\n",
        "        )\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Use Xavier initialization for GELU\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            nn.init.ones_(module.weight)\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5cCpY3Z_0UI"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import Callback\n",
        "\n",
        "class MyCustomCallback(Callback):\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        print(\"Epoch ended!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlcuWj53kAm7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CosineClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(output_dim, input_dim))  # 可学习的权重\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 计算 Cosine Similarity\n",
        "        x_norm = F.normalize(x, dim=1)  # 对输入进行 L2 归一化\n",
        "        w_norm = F.normalize(self.weight, dim=1)  # 对权重进行 L2 归一化\n",
        "        cosine_sim = torch.matmul(x_norm, w_norm.T)  # [batch_size, output_dim]\n",
        "        return cosine_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WJrsEc-kRrx"
      },
      "outputs": [],
      "source": [
        "# 偏置cosine sim\n",
        "class CosineClassifierWithBias(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(output_dim, input_dim))\n",
        "        self.bias = nn.Parameter(torch.zeros(output_dim))  # 偏置\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = F.normalize(x, dim=1)\n",
        "        w_norm = F.normalize(self.weight, dim=1)\n",
        "        cosine_sim = torch.matmul(x_norm, w_norm.T)\n",
        "        return cosine_sim + self.bias\n",
        "    def apply_weight(self, weight):\n",
        "        with torch.no_grad():\n",
        "            self.weight.copy_(weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZARdnJF5auH"
      },
      "outputs": [],
      "source": [
        "class MemeBLIP(pl.LightningModule):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # 动态线性投影\n",
        "        self.image_projection = LinearProjection(\n",
        "            input_dim=1024,\n",
        "            output_dim=cfg.map_dim,\n",
        "            num_layers=1,\n",
        "            drop_probs=cfg.drop_probs\n",
        "        ).to(self.cfg.device)\n",
        "\n",
        "        self.text_projection = LinearProjection(\n",
        "            input_dim=768,\n",
        "            output_dim=cfg.map_dim,\n",
        "            num_layers=1,\n",
        "            drop_probs=cfg.drop_probs\n",
        "        ).to(self.cfg.device)\n",
        "\n",
        "        # Adapter\n",
        "        self.image_adapter = Adapter(cfg.map_dim, reduction=4).to(self.cfg.device)\n",
        "        self.text_adapter = Adapter(cfg.map_dim, reduction=4).to(self.cfg.device)\n",
        "        self.pre_output_layer = nn.Sequential(\n",
        "            nn.Linear(cfg.map_dim, 512),  # 第一层：压缩特征维度\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, cfg.hidden_dim),  # 第二层：映射到分类器输入维度\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5)\n",
        "        )\n",
        "        # 加载 BLIP 模型和处理器\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-large-coco\")\n",
        "        self.model = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-large-coco\").to(cfg.device)\n",
        "\n",
        "        self.map_dim = cfg.map_dim  # BLIP 模型的隐藏层大小\n",
        "\n",
        "        # 分类器\n",
        "        # 定义非线性分类头\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(cfg.hidden_dim, 512),  # classifier_0\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=0.5),  # 增加 Dropout\n",
        "            CosineClassifierWithBias(512, 256),             # classifier_2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=0.5),  # 增加 Dropout\n",
        "            CosineClassifierWithBias(256, cfg.num_classes)  # classifier_4\n",
        "        )\n",
        "        # 初始化分类器权重\n",
        "        self.init_head_text_feat()\n",
        "\n",
        "        # 损失函数\n",
        "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # 评估指标\n",
        "        self.acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=cfg.num_classes)\n",
        "        self.auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=cfg.num_classes)\n",
        "        self.f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=cfg.num_classes)\n",
        "        self.model = self.model.to(cfg.device)\n",
        "        self.classifier = self.classifier.to(cfg.device)\n",
        "        # 存储梯度\n",
        "        self.gradients = {}\n",
        "\n",
        "    def save_gradient(self, name):\n",
        "        # 定义保存梯度的钩子\n",
        "        def hook(module, grad_input, grad_output):\n",
        "            self.gradients[name] = grad_output[0].detach()\n",
        "        return hook\n",
        "\n",
        "    def register_hooks(self):\n",
        "        # 为关键层注册钩子\n",
        "        self.image_projection.proj.register_backward_hook(self.save_gradient(\"image_projection\"))\n",
        "        self.text_projection.proj.register_backward_hook(self.save_gradient(\"text_projection\"))\n",
        "        self.image_adapter.fc.register_backward_hook(self.save_gradient(\"image_adapter\"))\n",
        "        self.text_adapter.fc.register_backward_hook(self.save_gradient(\"text_adapter\"))\n",
        "        self.pre_output_layer.register_backward_hook(self.save_gradient(\"pre_output_layer\"))\n",
        "\n",
        "        for i, layer in enumerate(self.classifier):\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                layer.register_backward_hook(self.save_gradient(f\"classifier_{i}\"))\n",
        "\n",
        "    def init_head_text_feat(self):\n",
        "        print(\"Initialize head with text features\")\n",
        "\n",
        "        template = \"a photo of a {}.\"\n",
        "        prompts = [template.format(c.replace(\"_\", \" \")) for c in self.cfg.class_names]\n",
        "\n",
        "        prompts = self.processor.tokenizer(\n",
        "            prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "        ).to(self.cfg.device)\n",
        "\n",
        "        prompts = {k: v for k, v in prompts.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
        "\n",
        "        text_features = self.model.text_encoder(**prompts).last_hidden_state.mean(dim=1)\n",
        "\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "\n",
        "        if hasattr(self.classifier[-1], 'apply_weight'):\n",
        "            if text_features.size(1) != 256:\n",
        "              projection = nn.Linear(text_features.size(1), 256).to(self.cfg.device)\n",
        "              text_features1 = projection(text_features)\n",
        "              self.classifier[-1].apply_weight(text_features1)\n",
        "        else:\n",
        "            print(\"Warning: Classifier -1 does not have 'apply_weight' method. Skipping initialization.\")\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # 提取特征\n",
        "        image_features = batch['image_features']\n",
        "        text_features = batch['text_features']\n",
        "\n",
        "        if isinstance(image_features, tuple):\n",
        "          image_features = image_features[0].to(self.cfg.device)\n",
        "        if isinstance(text_features, tuple):\n",
        "          text_features = text_features[0].to(self.cfg.device)\n",
        "\n",
        "        # Linear Projection\n",
        "        image_proj = self.image_projection(image_features).to(self.cfg.device)\n",
        "        text_proj = self.text_projection(text_features).to(self.cfg.device)\n",
        "\n",
        "        # Adapter\n",
        "        adapted_image = self.image_adapter(image_proj).to(self.cfg.device)\n",
        "        adapted_text = self.text_adapter(text_proj).to(self.cfg.device)\n",
        "\n",
        "        text_adapted_features = self.cfg.ratio  * adapted_text + (1 - self.cfg.ratio ) * text_proj\n",
        "        image_adapted_features = self.cfg.ratio  * adapted_image + (1 - self.cfg.ratio ) * image_proj\n",
        "\n",
        "        image_adapted_features = image_adapted_features / image_adapted_features.norm(dim=-1, keepdim=True)\n",
        "        text_adapted_features = text_adapted_features / text_adapted_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # 特征融合\n",
        "        combined_features = torch.mul(image_adapted_features, text_adapted_features).to(self.cfg.device)\n",
        "\n",
        "        # Pre-Output Transformation\n",
        "        pre_output_features = self.pre_output_layer(combined_features).to(self.cfg.device)\n",
        "\n",
        "        logits = self.classifier(pre_output_features).squeeze(dim=1).to(self.cfg.device)\n",
        "        return logits\n",
        "\n",
        "    def common_step(self, batch):\n",
        "        logits = self.forward(batch)  # 使用分类器的输出\n",
        "        preds_proxy = torch.sigmoid(logits)\n",
        "        _ , preds = logits.data.max(1)\n",
        "        loss = self.cross_entropy_loss(logits, batch[\"labels\"])  # 标签大小为 [batch_size]\n",
        "        # preds = torch.argmax(logits, dim=-1)\n",
        "        acc = self.acc(preds, batch[\"labels\"])\n",
        "        auroc = self.auroc(preds_proxy, batch['labels'])\n",
        "        f1 = self.f1(preds, batch[\"labels\"])\n",
        "        return {\"loss\": loss, \"acc\": acc, \"auroc\": auroc, \"f1\": f1}\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        logits = self.forward(batch)\n",
        "        loss = self.cross_entropy_loss(logits, batch[\"labels\"])\n",
        "        preds_proxy = torch.sigmoid(logits)\n",
        "        _ , preds = logits.data.max(1)\n",
        "        # preds = torch.argmax(logits, dim=-1)\n",
        "        acc = self.acc(preds, batch[\"labels\"])\n",
        "        auroc = self.auroc(preds_proxy, batch['labels'])\n",
        "        f1 = self.f1(preds, batch[\"labels\"])\n",
        "\n",
        "\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_auroc\", auroc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_f1\", f1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.cfg.lr, weight_decay=self.cfg.weight_decay)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # 前向传播\n",
        "        logits = self.forward(batch)\n",
        "        loss = self.cross_entropy_loss(logits, batch[\"labels\"])\n",
        "\n",
        "        preds_proxy = torch.sigmoid(logits)\n",
        "        _ , preds = logits.data.max(1)\n",
        "        # 预测和计算指标\n",
        "        # preds = torch.argmax(logits, dim=-1)\n",
        "        acc = self.acc(preds, batch[\"labels\"])\n",
        "        auroc = self.auroc(torch.softmax(logits, dim=-1), batch[\"labels\"])\n",
        "        f1 = self.f1(preds, batch[\"labels\"])\n",
        "\n",
        "        # 日志记录\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        self.log(\"val_auroc\", auroc, prog_bar=True)\n",
        "        self.log(\"val_f1\", f1, prog_bar=True)\n",
        "\n",
        "        return {\"loss\": loss, \"acc\": acc, \"auroc\": auroc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3b9Nj1U6oYh"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # # 建议的超参数\n",
        "    # cfg.lr = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
        "    # cfg.batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    # cfg.num_pre_output_layers = trial.suggest_int(\"num_pre_output_layers\", 1, 3)\n",
        "    # cfg.hidden_dim = trial.suggest_int(\"hidden_dim\", 128, 1024, step=128)\n",
        "    # cfg.drop_probs = [trial.suggest_uniform(f\"drop_prob_{i}\", 0.1, 0.5) for i in range(3)]\n",
        "    # cfg.num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    # cfg.activation = trial.suggest_categorical(\"activation\", [\"ReLU\", \"LeakyReLU\", \"GELU\"])\n",
        "    # cfg.hidden_dim1 = trial.suggest_categorical(\"hidden_dim1\", [256, 512, 1024])\n",
        "\n",
        "\n",
        "    # 创建模型\n",
        "    model = MemeBLIP(cfg)\n",
        "\n",
        "    # Trainer 配置\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=cfg.max_epochs,\n",
        "        accelerator=\"gpu\",\n",
        "        devices=len(cfg.gpus),\n",
        "        logger=pl.loggers.TensorBoardLogger(\"logs/\"),\n",
        "        callbacks=[pl.callbacks.ModelCheckpoint(dirpath=\"checkpoints/\")]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        validation_metrics = trainer.callback_metrics\n",
        "        return validation_metrics[\"val_loss\"].item()  # 优化目标\n",
        "    except Exception as e:\n",
        "        print(f\"Trial failed: {e}\")\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "# 创建 Optuna study 并运行优化\n",
        "# study = optuna.create_study(direction=\"minimize\")\n",
        "# study.optimize(objective, n_trials=20)\n",
        "\n",
        "# print(\"Best hyperparameters:\", study.best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZmG1kBh5RJu"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_uniform_(module.weight)  # Xavier 初始化\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MCp_FLc2xQP"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,  # 如果验证集损失在 3 个 epoch 内没有改善，则停止训练\n",
        "    verbose=True,\n",
        "    mode=\"min\"\n",
        ")\n",
        "\n",
        "model = MemeBLIP(cfg)\n",
        "model.register_hooks()\n",
        "model.apply(initialize_weights)\n",
        "model.to(cfg.device)\n",
        "for name, param in model.classifier.named_parameters():\n",
        "    print(f\"{name}: {param.data}\")\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params': model.image_adapter.parameters(), 'lr': 1e-3},\n",
        "    {'params': model.text_adapter.parameters(), 'lr': 1e-3},\n",
        "])\n",
        "for batch in train_loader:\n",
        "    model.forward(batch)  # 输出隐藏状态形状\n",
        "    break\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=cfg.max_epochs,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=len(cfg.gpus),\n",
        "    logger=pl.loggers.TensorBoardLogger(\"logs/\")\n",
        ")\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(log_dir='logs/gradient_logs')\n",
        "\n",
        "for name, grad in model.gradients.items():\n",
        "    writer.add_histogram(f\"Gradients/{name}\", grad, global_step=self.current_epoch)\n",
        "\n",
        "\n",
        "# 检查模型输出\n",
        "logits = model.forward(batch)\n",
        "print(f\"logits shape: {logits.shape}\")\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "validation_metrics = trainer.validate(model, val_loader, verbose=True)\n",
        "print(\"Validation Metrics:\", validation_metrics)\n",
        "print(\"Validation Accuracy:\", trainer.callback_metrics[\"val_acc\"])\n",
        "print(\"Validation AUROC:\", trainer.callback_metrics[\"val_auroc\"])\n",
        "print(\"Validation F1 Score:\", trainer.callback_metrics[\"val_f1\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgxRgM9lm9xt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_gradient_distribution(grad, layer_name):\n",
        "    plt.hist(grad.cpu().numpy().flatten(), bins=50)\n",
        "    plt.title(f\"Gradient Distribution - {layer_name}\")\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "for name, grad in model.gradients.items():\n",
        "    plot_gradient_distribution(grad, name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1IyVCRJLYrPTP2hntH4XqgHVBKlWLCq1I",
      "authorship_tag": "ABX9TyPwRYZ8fQ5/6d4tdcqgBo8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}